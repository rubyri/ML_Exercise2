{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fb92b7",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import arff\n",
    "from pathlib import Path\n",
    "\n",
    "# --- File paths ---\n",
    "data_dir = Path().resolve()\n",
    "file1 = data_dir / \"analcatdata_creditscore.arff\"\n",
    "file2 = data_dir / \"dataset_54_vehicle.arff\"\n",
    "\n",
    "# --- Load ARFF datasets ---\n",
    "def load_arff_to_df(file_path):\n",
    "    data, meta = arff.loadarff(file_path)\n",
    "    df = pd.DataFrame(data)\n",
    "    # decode byte strings if needed\n",
    "    for col in df.select_dtypes([object]).columns:\n",
    "        df[col] = df[col].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
    "    return df\n",
    "\n",
    "df_credit = load_arff_to_df(file1)\n",
    "df_vehicle = load_arff_to_df(file2)\n",
    "\n",
    "print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9364bb",
   "metadata": {},
   "source": [
    "### Data exploration and preprocessing\n",
    "\n",
    "(missing values, outliers, scaling, encoding, etc.)\n",
    "\n",
    "#### Credit Score Dataset Overview\n",
    "Entries:    100  \n",
    "Columns:    7  \n",
    "Response variable: \"Application.accepted\"  \n",
    "Missing values: No  \n",
    "Data types: float, object (binary)\n",
    "\n",
    "| Column | Type | Observations |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Age | numeric | Range 20-55, mean ~32 |\n",
    "| Income.per.dependent | numeric | \n",
    "| Monthly.credit.card.exp | numeric | Range 0-1898, mean~189 |\n",
    "| Own.home | categorical | binary(0,1); 64 yes, 36 no |\n",
    "| Self.employed | categorical | binary(0,1); 95 no, 5 yes |\n",
    "| Derogatory.reports | categorical | range 0-7, 82 no |\n",
    "| **Application.accepted** | categorical | binary(0,1); 73 yes, 27 no |\n",
    "\n",
    "Considerations for Pre-Processing: \n",
    "- The categorical values are currently stored as String ('0', '1')\n",
    "- The numeric values should be scaled. \n",
    "- Self.employed, Derogatory.reports and Application.accepted are skewed\n",
    "- Monthly.credit.card.exp already shows outliers (max 1898 vs mean 189)\n",
    "- There are no missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Credit Score Dataset Overview ---\n",
    "\n",
    "display(df_credit.info())       # 100 entries, 7 columns; dtypes: float64(3), object(4)\n",
    "print(df_credit.isnull().sum())\n",
    "display(df_credit.head())\n",
    "print(df_credit.describe(include='all').transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings to integers \n",
    "string_cols = [\"Own.home\", \"Self.employed\", \"Application.accepted\", \"Derogatory.reports\"]\n",
    "\n",
    "for col in string_cols:\n",
    "    df_credit[col] = pd.to_numeric(df_credit[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# # Verify conversion\n",
    "# for col in string_cols:\n",
    "#     print(f\"{col}: {df_credit[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualization of categorical features distribution ---\n",
    "cols_to_plot = [\"Own.home\", \"Self.employed\", \"Application.accepted\"]\n",
    "colors = {0: \"#ff814f\", 1: \"#66c2a5\"}  \n",
    "\n",
    "counts = {col: df_credit[col].value_counts().sort_index() for col in cols_to_plot}\n",
    "counts_df = pd.DataFrame(counts).T.fillna(0).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "counts_df.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    color=colors,\n",
    "    ax=ax,\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_title(\"Distribution of Categorical Features\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha=\"center\")       # rotate x-axis labels\n",
    "ax.legend(title=\"Value\", loc=\"upper right\")\n",
    "\n",
    "for container in ax.containers:                                         # count labels\n",
    "    ax.bar_label(container, label_type='center', color=\"white\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8643e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlation Heatmap ---\n",
    "numeric_df = df_credit.select_dtypes(include=[np.number])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "corr = numeric_df.corr()\n",
    "sns.heatmap(\n",
    "    corr, \n",
    "    annot=True, \n",
    "    cmap=\"coolwarm\", \n",
    "    fmt=\".2f\", \n",
    "    square=True)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Pairplot ---\n",
    "pairplot_features = [\"Age\", \"Income.per.dependent\", \"Monthly.credit.card.exp\", \"Derogatory.reports\"]\n",
    "sns.pairplot(\n",
    "    df_credit[pairplot_features + [\"Application.accepted\"]], \n",
    "    hue=\"Application.accepted\", \n",
    "    diag_kind=\"kde\", \n",
    "    palette={1: \"#66c2a5\", 0: \"#fc8d62\"} )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897d86b",
   "metadata": {},
   "source": [
    "\n",
    "#### Vehicle Dataset Overview\n",
    "Entries: 846  \n",
    "Columns: 19  \n",
    "Response variable: \"class\"  \n",
    "Missing values: No  \n",
    "Data types: float, object (String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vehicle Dataset Overview ---\n",
    "display(df_vehicle.info())   # 846 entries, 19 columns; dtypes: float64(18), object(1)\n",
    "print(df_vehicle.isnull().sum())\n",
    "display(df_vehicle.head())   \n",
    "(df_vehicle.describe(include='all').transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612b80f",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Carry out the classification:\n",
    "- Run classifiers, and Experiment with:\n",
    "    - Different classifiers and your datasets\n",
    "    - Different parameter settings (= several results per classifier per dataset, not only\n",
    "random/best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0: Logistic Regression model with all predictors and no scaling/data cleaning beforehand\n",
    "# Later to be used as comparison\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "X = df_credit.drop(columns=[\"Application.accepted\"])        # predictors\n",
    "y = df_credit[\"Application.accepted\"]                       # response\n",
    "\n",
    "# Split data: 2/3 train, 1/3 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=1/3,       \n",
    "    random_state=42,     # seed\n",
    "    stratify=y           \n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Fit model\n",
    "model_logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model_logreg.predict(X_test)\n",
    "y_prob = model_logreg.predict_proba(X_test)[:, 1]  # probability of class 1\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afa96e",
   "metadata": {},
   "source": [
    "##### Model 0 report: \n",
    "\n",
    "The model has 100% accuracy which means there have to be issues with the data. This is also visible in the confusion matrix: The model only predicted true negatives (9) and true positives (25), but neither false negatives nor false positives.\n",
    "\n",
    "The likely reason is that small size of the data set. Some variables (like derogatory.reports) have such a strong correlation with the response variable that they are essentially deterministic predictors - the model only needs to know these variables to make a correct prediction about whether an application was accepted or not.\n",
    "\n",
    "Looking at the mean values of accepted and rejected applications we can see that the monthly.credit.card.exp for rejected applications is *0* and the derogatory reports vary wiledly (0.1 vs 1+).\n",
    "\n",
    "Looking at the two suspicious variables further it becomes clear that the monthly.credit.card.exp is a very strong indicator for whether the application gets accepted or not - *all* 27 rejected applications have a value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd725f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.groupby(\"Application.accepted\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.groupby(\"Application.accepted\")[[\"Monthly.credit.card.exp\", \"Derogatory.reports\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dc84c",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "Evaluate and analyse the performance (primarily effectiveness, but also provide basic\n",
    "details on efficiency):\n",
    "- Choose suitable, multiple performance measures\n",
    "- Make valid comparisons (among the classifiers, across your datasets, parameters,\n",
    "preprocessing effects...)\n",
    "- (How) can you improve the results?\n",
    "- Can you identify any patterns/trends?\n",
    "    - Which methods work well and which did not, is there e.g. one method\n",
    "outperforming the others on all datasets?\n",
    "    - How do the results change when preprocessing strategies change? How sensitive\n",
    "is an algorithm to parameter settings?\n",
    "    - Are there differences across the datasets? Design your experiments so that you\n",
    "can investigate the influence of single parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc29d58",
   "metadata": {},
   "source": [
    "### Holdout vs Cross-Validation\n",
    "\n",
    "- Pay attention to your splits and settings\n",
    "Are there differences? Why? In which metrics? What could have caused it?\n",
    "- Compare/document changes in runtime behaviour with the changing e.g. dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ee6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "babd2907",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
