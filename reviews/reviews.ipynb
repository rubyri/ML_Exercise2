{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac883d001f8e716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:07:06.915277Z",
     "start_time": "2025-11-10T10:07:06.902144Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59889a4ba8f868de",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:07:34.444780Z",
     "start_time": "2025-11-10T10:07:32.035548Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 10002)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V9992</th>\n",
       "      <th>V9993</th>\n",
       "      <th>V9994</th>\n",
       "      <th>V9995</th>\n",
       "      <th>V9996</th>\n",
       "      <th>V9997</th>\n",
       "      <th>V9998</th>\n",
       "      <th>V9999</th>\n",
       "      <th>V10000</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Shea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Riley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Chachra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Agresti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nigam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  V1  V2  V3  V4  V5  V6  V7  V8  V9  ...  V9992  V9993  V9994  V9995  \\\n",
       "0   0  17   4   8   8   9   4   0   2   3  ...      0      0      0      0   \n",
       "1   1  21   9   5   8   6   2  16   3  12  ...      0      0      0      2   \n",
       "2   2   9   7   6   3   8   2   9   4   4  ...      0      0      0      0   \n",
       "3   3   8   3   5   2   4   3   8   2   4  ...      0      0      1      0   \n",
       "4   4  15   8   8   4   7   8   4   7   1  ...      0      0      0      0   \n",
       "\n",
       "   V9996  V9997  V9998  V9999  V10000    Class  \n",
       "0      0      0      0      1       1     Shea  \n",
       "1      2      1      0      1       0    Riley  \n",
       "2      0      0      0      1       1  Chachra  \n",
       "3      1      0      0      0       0  Agresti  \n",
       "4      0      0      0      0       0    Nigam  \n",
       "\n",
       "[5 rows x 10002 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PATH = \"amazon_review_ID.shuf.lrn.csv\"\n",
    "TEST_PATH = \"amazon_review_ID.shuf.tes.csv\"\n",
    "ID_COL = \"ID\"\n",
    "TARGET_COL = \"Class\"\n",
    "SUBMIT_OUT = \"submission.csv\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43431ec8b42c4754",
   "metadata": {},
   "source": [
    "750 rows and 10,002 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065bae24ae94dd9",
   "metadata": {},
   "source": [
    "**Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f04205370c4756f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:07:42.679949Z",
     "start_time": "2025-11-10T10:07:37.157811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: 0\n",
      "Duplicate rows: 0\n",
      "Constant columns (same value for all rows): 0\n"
     ]
    }
   ],
   "source": [
    "# Basic hygiene\n",
    "df.columns = df.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "missing_train = df.isnull().sum()\n",
    "n_missing_train = (missing_train > 0).sum()\n",
    "\n",
    "print(f\"Columns with missing values: {n_missing_train}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "constant_cols = [c for c in df.columns if df[c].nunique() == 1]\n",
    "print(f\"Constant columns: {len(constant_cols)}\")\n",
    "\n",
    "for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f271094ac3345e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:07:46.003211Z",
     "start_time": "2025-11-10T10:07:45.390722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature grouping\n",
    "binary_thresh = 2\n",
    "low_thresh = 10\n",
    "nunique = df.nunique()\n",
    "\n",
    "constant = nunique[nunique == 1].index.tolist()\n",
    "binary = nunique[nunique == binary_thresh].index.tolist()\n",
    "low_card = nunique[(nunique > binary_thresh) & (nunique <= low_thresh)].index.tolist()\n",
    "continuous = nunique[nunique > low_thresh].index.tolist()\n",
    "\n",
    "groups = {\"constant\": constant, \"binary\": binary, \"categorical\": low_card, \"numeric\": continuous}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b068aa9ebff5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:07:50.340372Z",
     "start_time": "2025-11-10T10:07:50.006942Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[TARGET_COL])\n",
    "X = df.drop(columns=[TARGET_COL, ID_COL], errors=\"ignore\")\n",
    "X_test_raw = test.drop(columns=[ID_COL], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c7e30477207cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:07:56.030021Z",
     "start_time": "2025-11-10T10:07:55.996307Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in groups:\n",
    "    groups[key] = [col for col in groups[key] if col in X.columns]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"binary\", \"passthrough\", groups[\"binary\"]),\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), groups[\"categorical\"]),\n",
    "        (\"numeric\", RobustScaler(), groups[\"numeric\"])\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "feature_selection = SelectKBest(score_func=mutual_info_classif, k=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39de06bb44d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "models = {\n",
    "    \"LogReg\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            penalty=\"elasticnet\", solver=\"saga\", class_weight=\"balanced\",\n",
    "            l1_ratio=0.5, C=0.5, max_iter=1000, n_jobs=-1, random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", C=0.6, gamma=\"scale\", probability=False, random_state=42))\n",
    "    ]),\n",
    "    \"DecisionTree\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", DecisionTreeClassifier(criterion=\"gini\", max_depth=20, min_samples_split=10,\n",
    "                                       min_samples_leaf=5, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\")\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross Validation\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "results = []\n",
    "for name, pipe in models.items():\n",
    "    cv_res = cross_validate(pipe, X, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV_Accuracy\": np.mean(cv_res[\"test_accuracy\"]),\n",
    "        \"CV_MacroF1\": np.mean(cv_res[\"test_macro_f1\"])\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"CV_MacroF1\", ascending=False)\n",
    "print(\"\\nModel selection (CV on training data):\")\n",
    "print(res_df.to_string(index=False))\n",
    "\n",
    "best_name = res_df.iloc[0][\"Model\"]\n",
    "print(f\"\\nSelected best model by CV Macro-F1: {best_name}\")\n",
    "\n",
    "best_pipeline = models[best_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee3ffcdcf58798",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "806e9892ecba80b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T10:37:18.130369Z",
     "start_time": "2025-11-10T10:15:43.929269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model  CV_Accuracy  CV_MacroF1\n",
      "0  RandomForest         0.68    0.624086\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\")\n",
    "}\n",
    "\n",
    "# Define Random Forest pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"select\", feature_selection),  # your existing SVD or feature selection step\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_res = cross_validate(\n",
    "    rf_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# Summarize results\n",
    "res_df = pd.DataFrame([{\n",
    "    \"Model\": \"RandomForest\",\n",
    "    \"CV_Accuracy\": np.mean(cv_res[\"test_accuracy\"]),\n",
    "    \"CV_MacroF1\": np.mean(cv_res[\"test_macro_f1\"])\n",
    "}])\n",
    "\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68484030a1c55d0",
   "metadata": {},
   "source": [
    "**Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2a380ca223021",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-10T10:46:02.295041Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "base_learners = [\n",
    "    (\"logreg\", Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            penalty=\"elasticnet\", solver=\"saga\", class_weight=\"balanced\",\n",
    "            l1_ratio=0.5, C=0.5, max_iter=1000, n_jobs=-1, random_state=42\n",
    "        ))\n",
    "    ])),\n",
    "    (\"svm\", Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", C=0.6, gamma=\"scale\", probability=True, random_state=42))\n",
    "    ])),\n",
    "    (\"tree\", Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", DecisionTreeClassifier(\n",
    "            criterion=\"gini\", max_depth=20, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=42\n",
    "        ))\n",
    "    ])),\n",
    "    (\"rf\", Pipeline([\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=5,\n",
    "            min_samples_leaf=2, class_weight=\"balanced_subsample\",\n",
    "            n_jobs=-1, random_state=42\n",
    "        ))\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# stacking ensemble\n",
    "meta_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "stacked_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_model,\n",
    "    stack_method=\"predict_proba\",  \n",
    "    n_jobs=-1,\n",
    "    passthrough=False               \n",
    ")\n",
    "\n",
    "#cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\")\n",
    "}\n",
    "\n",
    "cv_res = cross_validate(\n",
    "    stacked_clf,\n",
    "    X, y,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "res_df = pd.DataFrame([{\n",
    "    \"Model\": \"Stacked Ensemble\",\n",
    "    \"CV_Accuracy\": np.mean(cv_res[\"test_accuracy\"]),\n",
    "    \"CV_MacroF1\": np.mean(cv_res[\"test_macro_f1\"])\n",
    "}])\n",
    "\n",
    "print(\"\\nStacked Ensemble Performance:\")\n",
    "print(res_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f6a501f73fe05",
   "metadata": {},
   "source": [
    "### Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d962e306c66614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df.nunique()\n",
    "threshold = 16\n",
    "\n",
    "unique_counts_grouped = unique_counts.apply(lambda x: x if x <= threshold else threshold + 1)\n",
    "value_distribution = unique_counts_grouped.value_counts().sort_index()\n",
    "\n",
    "# rename the last bin to \">threshold\"\n",
    "value_distribution.index = [\n",
    "    str(i) if i <= threshold else f\">{threshold}\" for i in value_distribution.index\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(value_distribution.index, value_distribution.values, color=\"steelblue\")\n",
    "plt.xlabel(\"Number of unique values per feature\")\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.title(\"Distribution of feature uniqueness (grouped)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3689ff71463d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.info()\n",
    "df.describe().T\n",
    "\n",
    "df[\"Class\"].value_counts().plot(kind=\"bar\", figsize=(12, 4), color=\"salmon\")\n",
    "plt.title(\"Reviews per Author\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "df.var().sort_values().head(10)  # lowest variance features\n",
    "df.var().sort_values(ascending=False).head(10)  # highest variance\n",
    "\n",
    "sampled = df.sample(axis=1, n=20, random_state=42)\n",
    "corr = sampled.corr()\n",
    "plt.imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation between 20 random features\")\n",
    "plt.show()\n",
    "\n",
    "df.groupby(\"class\").mean().T.var(axis=1).sort_values(ascending=False).head(10)\n",
    "\n",
    "feature = \"V1234\"  # choose one from above\n",
    "df.boxplot(column=feature, by=\"Class\", figsize=(10, 4))\n",
    "plt.title(f\"Distribution of {feature} across authors\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754f4937df69fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.mean(np.abs(coef), axis=0)\n",
    "top_idx = np.argsort(importance)[-20:]  # top 20\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(np.array(feature_names)[top_idx], importance[top_idx], color='tomato')\n",
    "plt.title(\"Top 20 most influential features (Logistic Regression)\")\n",
    "plt.xlabel(\"Mean absolute coefficient\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb169dca4bdd4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name in le.classes_:\n",
    "    mask = df[\"Class\"] == name\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], alpha=0.6, label=name)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.title(\"PCA projection of review features by author\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b4f620656c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression importance (mean absolute coefficient)\n",
    "log_reg_importance = np.mean(np.abs(model.coef_), axis=0)\n",
    "\n",
    "# LightGBM importance (gain-based)\n",
    "lgb_importance = lgb.feature_importances_\n",
    "\n",
    "# Combine into a dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"log_reg\": log_reg_importance,\n",
    "    \"lightgbm\": lgb_importance\n",
    "})\n",
    "\n",
    "# Normalize both scales for comparison\n",
    "importance_df[\"log_reg_norm\"] = importance_df[\"log_reg\"] / importance_df[\"log_reg\"].max()\n",
    "importance_df[\"lightgbm_norm\"] = importance_df[\"lightgbm\"] / importance_df[\"lightgbm\"].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791217d14ada7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between importances\n",
    "corr = importance_df[[\"log_reg_norm\", \"lightgbm_norm\"]].corr().iloc[0, 1]\n",
    "print(\"Correlation between model importances:\", round(corr, 3))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(importance_df[\"log_reg_norm\"], importance_df[\"lightgbm_norm\"], alpha=0.5, color='tomato')\n",
    "plt.xlabel(\"Logistic Regression importance (normalized)\")\n",
    "plt.ylabel(\"LightGBM importance (normalized)\")\n",
    "plt.title(f\"Feature Importance Correlation (r={corr:.2f})\")\n",
    "plt.show()\n",
    "\n",
    "# Show top features by either model\n",
    "top = importance_df.sort_values(\"lightgbm_norm\", ascending=False).head(20)\n",
    "top.plot(x=\"feature\", y=[\"log_reg_norm\", \"lightgbm_norm\"], kind=\"barh\", figsize=(10, 6), color=[\"#e07b7b\", \"#7bb8e0\"])\n",
    "plt.title(\"Top 20 Features by LightGBM vs Logistic Regression\")\n",
    "plt.xlabel(\"Normalized Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b94b4e3a412ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by both models' normalized importance\n",
    "importance_df_sorted = importance_df.sort_values(\"lightgbm_norm\", ascending=False)\n",
    "\n",
    "# Compute difference between models\n",
    "importance_df_sorted[\"diff\"] = importance_df_sorted[\"lightgbm_norm\"] - importance_df_sorted[\"log_reg_norm\"]\n",
    "\n",
    "# Features LightGBM finds much more important\n",
    "strong_lgb = importance_df_sorted.nlargest(15, \"diff\")[[\"feature\", \"lightgbm_norm\", \"log_reg_norm\", \"diff\"]]\n",
    "\n",
    "# Features Logistic Regression finds more important\n",
    "strong_log = importance_df_sorted.nsmallest(15, \"diff\")[[\"feature\", \"lightgbm_norm\", \"log_reg_norm\", \"diff\"]]\n",
    "\n",
    "# Plot LightGBM-dominant features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(strong_lgb[\"feature\"], strong_lgb[\"lightgbm_norm\"], color=\"#7bb8e0\", label=\"LightGBM\")\n",
    "plt.barh(strong_lgb[\"feature\"], strong_lgb[\"log_reg_norm\"], color=\"#e07b7b\", alpha=0.6, label=\"LogReg\")\n",
    "plt.title(\"Features LightGBM finds more important\")\n",
    "plt.xlabel(\"Normalized Importance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Logistic Regression-dominant features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(strong_log[\"feature\"], strong_log[\"log_reg_norm\"], color=\"#e07b7b\", label=\"LogReg\")\n",
    "plt.barh(strong_log[\"feature\"], strong_log[\"lightgbm_norm\"], color=\"#7bb8e0\", alpha=0.6, label=\"LightGBM\")\n",
    "plt.title(\"Features Logistic Regression finds more important\")\n",
    "plt.xlabel(\"Normalized Importance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8f7ea6a111621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Example: visualize partial dependence of feature 'V6567' for one author class (e.g., 'Riley')\n",
    "target_class = list(le.classes_).index('Riley')  # change 'Riley' to any author name\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    lgb,\n",
    "    X,\n",
    "    ['V6567'],\n",
    "    target=target_class,\n",
    "    grid_resolution=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5338ab1b55c86ac",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f092d1374f7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748e6311895886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"ID\", \"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b438c4f87506a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232bb8f3fbc745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm', lgb),\n",
    "        ('logreg', logreg)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    stack_method='predict_proba',  # use class probabilities as input to meta model\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6302fb9bac42b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.fit(X_train, y_train)\n",
    "y_pred = stack.predict(X_test)\n",
    "\n",
    "print(\"Stacked model accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e701af673bf4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Stacked model accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "unique_labels = np.unique(y_test)  # only labels present in test data\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=unique_labels,\n",
    "    target_names=le.inverse_transform(unique_labels)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96355f893b246bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the final (meta) model\n",
    "meta_model = stack.final_estimator_\n",
    "\n",
    "coefs = np.abs(meta_model.coef_).mean(axis=0)\n",
    "\n",
    "base_model_names = [name for name, _ in stack.estimators_]\n",
    "n_classes = meta_model.coef_.shape[0]\n",
    "split_size = int(len(coefs) / len(base_model_names))\n",
    "weights = [coefs[i * split_size:(i + 1) * split_size].mean() for i in range(len(base_model_names))]\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
