{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "id": "ac883d001f8e716",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Loading Data**",
   "id": "59889a4ba8f868de"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TRAIN_PATH = \"amazon_review_ID.shuf.lrn.csv\"\n",
    "TEST_PATH = \"amazon_review_ID.shuf.tes.csv\"\n",
    "ID_COL = \"ID\"\n",
    "TARGET_COL = \"Class\"\n",
    "SUBMIT_OUT = \"submission.csv\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "750 rows and 10,002 columns\n",
   "id": "43431ec8b42c4754"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Pre-Processing**",
   "id": "a065bae24ae94dd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Basic hygiene\n",
    "df.columns = df.columns.str.strip()\n",
    "test.columns = test.columns.str.strip()\n",
    "\n",
    "missing_train = df.isnull().sum()\n",
    "n_missing_train = (missing_train > 0).sum()\n",
    "\n",
    "print(f\"Columns with missing values: {n_missing_train}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "constant_cols = [c for c in df.columns if df[c].nunique() == 1]\n",
    "print(f\"Constant columns (same value for all rows): {len(constant_cols)}\")\n",
    "if len(constant_cols) > 0:\n",
    "    print(\"  Examples:\", constant_cols[:10])\n",
    "\n",
    "for col in df.select_dtypes(include=[\"int\"]).columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n"
   ],
   "id": "9f04205370c4756f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature grouping\n",
    "binary_thresh = 2\n",
    "low_thresh = 10\n",
    "nunique = df.nunique()\n",
    "\n",
    "constant = nunique[nunique == 1].index.tolist()\n",
    "binary = nunique[nunique == binary_thresh].index.tolist()\n",
    "low_card = nunique[(nunique > binary_thresh) & (nunique <= low_thresh)].index.tolist()\n",
    "continuous = nunique[nunique > low_thresh].index.tolist()\n",
    "\n",
    "groups = {\"constant\": constant, \"binary\": binary, \"categorical\": low_card, \"numeric\": continuous}\n"
   ],
   "id": "53f271094ac3345e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[TARGET_COL])\n",
    "X = df.drop(columns=[TARGET_COL, ID_COL], errors=\"ignore\")\n",
    "X_test_raw = test.drop(columns=[ID_COL], errors=\"ignore\")"
   ],
   "id": "51b068aa9ebff5e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "for key in groups:\n",
    "    groups[key] = [col for col in groups[key] if col in X.columns]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"binary\", \"passthrough\", groups[\"binary\"]),\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), groups[\"categorical\"]),\n",
    "        (\"numeric\", RobustScaler(), groups[\"numeric\"])\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "feature_selection = SelectKBest(score_func=mutual_info_classif, k=500)\n"
   ],
   "id": "40c7e30477207cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Models to compare\n",
    "models = {\n",
    "    \"LogReg\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            penalty=\"elasticnet\", solver=\"saga\", class_weight=\"balanced\",\n",
    "            l1_ratio=0.5, C=0.5, max_iter=1000, n_jobs=-1, random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"SVM\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", C=0.6, gamma=\"scale\", probability=False, random_state=42))\n",
    "    ]),\n",
    "    \"DecisionTree\": Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", feature_selection),\n",
    "        (\"clf\", DecisionTreeClassifier(criterion=\"gini\", max_depth=20, min_samples_split=10,\n",
    "                                       min_samples_leaf=5, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"macro_f1\": make_scorer(f1_score, average=\"macro\")\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Internal validation (on train) to pick a winner\n",
    "# -----------------------------\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "\n",
    "results = []\n",
    "for name, pipe in models.items():\n",
    "    cv_res = cross_validate(pipe, X, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV_Accuracy\": np.mean(cv_res[\"test_accuracy\"]),\n",
    "        \"CV_MacroF1\": np.mean(cv_res[\"test_macro_f1\"])\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"CV_MacroF1\", ascending=False)\n",
    "print(\"\\nModel selection (CV on training data):\")\n",
    "print(res_df.to_string(index=False))\n",
    "\n",
    "best_name = res_df.iloc[0][\"Model\"]\n",
    "print(f\"\\nSelected best model by CV Macro-F1: {best_name}\")\n",
    "\n",
    "best_pipeline = models[best_name]\n"
   ],
   "id": "db39de06bb44d75c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add Random Forest to models\n",
    "models[\"RandomForest\"] = Pipeline([\n",
    "    (\"select\", feature_selection),  # reuse your SVD step\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "results = []\n",
    "for name, pipe in models.items():\n",
    "    cv_res = cross_validate(pipe, X, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV_Accuracy\": np.mean(cv_res[\"test_accuracy\"]),\n",
    "        \"CV_MacroF1\": np.mean(cv_res[\"test_macro_f1\"])\n",
    "    })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"CV_MacroF1\", ascending=False)\n",
    "print(res_df)\n"
   ],
   "id": "806e9892ecba80b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visual Analysis",
   "id": "696f6a501f73fe05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique_counts = df.nunique()\n",
    "threshold = 16\n",
    "\n",
    "unique_counts_grouped = unique_counts.apply(lambda x: x if x <= threshold else threshold + 1)\n",
    "value_distribution = unique_counts_grouped.value_counts().sort_index()\n",
    "\n",
    "# rename the last bin to \">threshold\"\n",
    "value_distribution.index = [\n",
    "    str(i) if i <= threshold else f\">{threshold}\" for i in value_distribution.index\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(value_distribution.index, value_distribution.values, color=\"steelblue\")\n",
    "plt.xlabel(\"Number of unique values per feature\")\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.title(\"Distribution of feature uniqueness (grouped)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "3d962e306c66614e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.info()\n",
    "df.describe().T\n",
    "\n",
    "df[\"Class\"].value_counts().plot(kind=\"bar\", figsize=(12, 4), color=\"salmon\")\n",
    "plt.title(\"Reviews per Author\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "df.var().sort_values().head(10)  # lowest variance features\n",
    "df.var().sort_values(ascending=False).head(10)  # highest variance\n",
    "\n",
    "sampled = df.sample(axis=1, n=20, random_state=42)\n",
    "corr = sampled.corr()\n",
    "plt.imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation between 20 random features\")\n",
    "plt.show()\n",
    "\n",
    "df.groupby(\"class\").mean().T.var(axis=1).sort_values(ascending=False).head(10)\n",
    "\n",
    "feature = \"V1234\"  # choose one from above\n",
    "df.boxplot(column=feature, by=\"Class\", figsize=(10, 4))\n",
    "plt.title(f\"Distribution of {feature} across authors\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()\n"
   ],
   "id": "92c3689ff71463d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importance = np.mean(np.abs(coef), axis=0)\n",
    "top_idx = np.argsort(importance)[-20:]  # top 20\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(np.array(feature_names)[top_idx], importance[top_idx], color='tomato')\n",
    "plt.title(\"Top 20 most influential features (Logistic Regression)\")\n",
    "plt.xlabel(\"Mean absolute coefficient\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3754f4937df69fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name in le.classes_:\n",
    "    mask = df[\"Class\"] == name\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], alpha=0.6, label=name)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.title(\"PCA projection of review features by author\")\n",
    "plt.show()"
   ],
   "id": "fb169dca4bdd4e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb.fit(X_train, y_train)"
   ],
   "id": "f97c5d85f14fb53b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Logistic Regression importance (mean absolute coefficient)\n",
    "log_reg_importance = np.mean(np.abs(model.coef_), axis=0)\n",
    "\n",
    "# LightGBM importance (gain-based)\n",
    "lgb_importance = lgb.feature_importances_\n",
    "\n",
    "# Combine into a dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"log_reg\": log_reg_importance,\n",
    "    \"lightgbm\": lgb_importance\n",
    "})\n",
    "\n",
    "# Normalize both scales for comparison\n",
    "importance_df[\"log_reg_norm\"] = importance_df[\"log_reg\"] / importance_df[\"log_reg\"].max()\n",
    "importance_df[\"lightgbm_norm\"] = importance_df[\"lightgbm\"] / importance_df[\"lightgbm\"].max()\n"
   ],
   "id": "f43b4f620656c4f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correlation between importances\n",
    "corr = importance_df[[\"log_reg_norm\", \"lightgbm_norm\"]].corr().iloc[0, 1]\n",
    "print(\"Correlation between model importances:\", round(corr, 3))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(importance_df[\"log_reg_norm\"], importance_df[\"lightgbm_norm\"], alpha=0.5, color='tomato')\n",
    "plt.xlabel(\"Logistic Regression importance (normalized)\")\n",
    "plt.ylabel(\"LightGBM importance (normalized)\")\n",
    "plt.title(f\"Feature Importance Correlation (r={corr:.2f})\")\n",
    "plt.show()\n",
    "\n",
    "# Show top features by either model\n",
    "top = importance_df.sort_values(\"lightgbm_norm\", ascending=False).head(20)\n",
    "top.plot(x=\"feature\", y=[\"log_reg_norm\", \"lightgbm_norm\"], kind=\"barh\", figsize=(10, 6), color=[\"#e07b7b\", \"#7bb8e0\"])\n",
    "plt.title(\"Top 20 Features by LightGBM vs Logistic Regression\")\n",
    "plt.xlabel(\"Normalized Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "791217d14ada7447",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sort by both models' normalized importance\n",
    "importance_df_sorted = importance_df.sort_values(\"lightgbm_norm\", ascending=False)\n",
    "\n",
    "# Compute difference between models\n",
    "importance_df_sorted[\"diff\"] = importance_df_sorted[\"lightgbm_norm\"] - importance_df_sorted[\"log_reg_norm\"]\n",
    "\n",
    "# Features LightGBM finds much more important\n",
    "strong_lgb = importance_df_sorted.nlargest(15, \"diff\")[[\"feature\", \"lightgbm_norm\", \"log_reg_norm\", \"diff\"]]\n",
    "\n",
    "# Features Logistic Regression finds more important\n",
    "strong_log = importance_df_sorted.nsmallest(15, \"diff\")[[\"feature\", \"lightgbm_norm\", \"log_reg_norm\", \"diff\"]]\n",
    "\n",
    "# Plot LightGBM-dominant features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(strong_lgb[\"feature\"], strong_lgb[\"lightgbm_norm\"], color=\"#7bb8e0\", label=\"LightGBM\")\n",
    "plt.barh(strong_lgb[\"feature\"], strong_lgb[\"log_reg_norm\"], color=\"#e07b7b\", alpha=0.6, label=\"LogReg\")\n",
    "plt.title(\"Features LightGBM finds more important\")\n",
    "plt.xlabel(\"Normalized Importance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Logistic Regression-dominant features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(strong_log[\"feature\"], strong_log[\"log_reg_norm\"], color=\"#e07b7b\", label=\"LogReg\")\n",
    "plt.barh(strong_log[\"feature\"], strong_log[\"lightgbm_norm\"], color=\"#7bb8e0\", alpha=0.6, label=\"LightGBM\")\n",
    "plt.title(\"Features Logistic Regression finds more important\")\n",
    "plt.xlabel(\"Normalized Importance\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "cd3b94b4e3a412ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Example: visualize partial dependence of feature 'V6567' for one author class (e.g., 'Riley')\n",
    "target_class = list(le.classes_).index('Riley')  # change 'Riley' to any author name\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    lgb,\n",
    "    X,\n",
    "    ['V6567'],\n",
    "    target=target_class,\n",
    "    grid_resolution=50\n",
    ")\n"
   ],
   "id": "43d8f7ea6a111621",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Smaller Model fo testing",
   "id": "e5338ab1b55c86ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lgb = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb.fit(X_train_s, y_train_s)\n",
    "y_pred = lgb.predict(X_test_s)\n"
   ],
   "id": "ce515b4162482ba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lgb = LGBMClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=31,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb.fit(\n",
    "    X_train_s, y_train_s,\n",
    "    eval_set=[(X_test_s, y_test_s)],\n",
    "    eval_metric='multi_logloss'\n",
    ")\n"
   ],
   "id": "54107503af58735f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stacking",
   "id": "f1c405f81f7a5e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "id": "33f092d1374f7c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df.drop(columns=[\"ID\", \"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42\n",
    ")\n"
   ],
   "id": "d748e6311895886f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lgb = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, n_jobs=-1)\n"
   ],
   "id": "17b438c4f87506a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm', lgb),\n",
    "        ('logreg', logreg)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    stack_method='predict_proba',  # use class probabilities as input to meta model\n",
    "    n_jobs=-1\n",
    ")\n"
   ],
   "id": "5232bb8f3fbc745c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stack.fit(X_train, y_train)\n",
    "y_pred = stack.predict(X_test)\n",
    "\n",
    "print(\"Stacked model accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ],
   "id": "e6302fb9bac42b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Stacked model accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "unique_labels = np.unique(y_test)  # only labels present in test data\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=unique_labels,\n",
    "    target_names=le.inverse_transform(unique_labels)\n",
    "))\n"
   ],
   "id": "4e701af673bf4622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the final (meta) model\n",
    "meta_model = stack.final_estimator_\n",
    "\n",
    "# The stacking layer combines outputs from each base model across all classes.\n",
    "# We'll look at the average absolute coefficient magnitude per base model.\n",
    "coefs = np.abs(meta_model.coef_).mean(axis=0)\n",
    "\n",
    "# Each base model contributes several columns (one per class),\n",
    "# so we aggregate by model\n",
    "base_model_names = [name for name, _ in stack.estimators_]\n",
    "n_classes = meta_model.coef_.shape[0]\n",
    "split_size = int(len(coefs) / len(base_model_names))\n",
    "weights = [coefs[i * split_size:(i + 1) * split_size].mean() for i in range(len(base_model_names))]\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / np.sum(weights)\n"
   ],
   "id": "96355f893b246bcc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
